{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](YOUR_COLAB_LINK_HERE)\n\n# 04 Comparison: CNNs vs Transformers\n## Objectives\n- Compare inductive biases and trade-offs.\n- Run a mini empirical comparison.\n- Discuss when to use which model.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Conceptual Primer: CNNs and Transformers\n\n### What is a CNN?\nA Convolutional Neural Network is a neural network designed for images. It uses **convolutions** to scan local regions with learnable filters. This builds in two key inductive biases: **locality** (nearby pixels matter most) and **translation equivariance** (a pattern can appear anywhere).\n\n**How it works**\n- **Convolution layers** learn filters that detect edges, textures, and shapes.\n- **Pooling** reduces spatial size and increases robustness to small shifts.\n- **Deeper layers** combine simpler patterns into higher-level features.\n- A **classifier head** maps features to labels.\n\nMathematically, convolution computes weighted sums over local patches; stacking layers grows the receptive field. CNNs are data-efficient because their priors match natural images.\n\n### What is a Transformer (ViT) for vision?\nA Vision Transformer treats an image as a **sequence of patch tokens**. Each patch is embedded into a vector, positional information is added, and **self-attention** lets every patch interact with every other patch.\n\n**Architecture overview**\n- **Patchify** the image (e.g., 16x16 patches).\n- **Linear embedding** of each patch.\n- **Positional embeddings** preserve spatial order.\n- **Transformer encoder blocks**: LayerNorm \u2192 Multi-Head Self-Attention \u2192 MLP, with residual connections.\n- **Class token** (or token pooling) feeds a classifier head.\n\nSelf-attention gives global context from the start. ViTs typically need large-scale data or pretraining, but transfer learning makes them practical for smaller datasets.\n\n### Key differences (intuition)\n- **Inductive bias**: CNNs assume locality; Transformers are more flexible but data-hungry.\n- **Context**: CNNs build global context gradually; Transformers use global attention immediately.\n- **Compute**: CNNs are efficient; attention cost grows with image size.\n\nReferences: ViT (https://arxiv.org/abs/2010.11929), Attention Is All You Need (https://arxiv.org/abs/1706.03762), ResNet (https://arxiv.org/abs/1512.03385).\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "!pip -q install torch torchvision transformers matplotlib"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Conceptual comparison"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "CNNs assume locality and translation equivariance.\nTransformers use global attention and scale with data and compute."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import time\nimport torch\nimport torchvision\nfrom torchvision import transforms\nfrom transformers import AutoImageProcessor, ViTForImageClassification\nimport matplotlib.pyplot as plt\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntransform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\ndataset = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=transform)\nloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=False)\nimages, labels = next(iter(loader))\nimages = images.to(device)\n\ncnn = torchvision.models.resnet18(pretrained=True).to(device)\nprocessor = AutoImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\nvit = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k').to(device)\n\nvit_inputs = processor(images=images.cpu(), return_tensors='pt')\nvit_inputs = {k: v.to(device) for k, v in vit_inputs.items()}\n\ndef throughput_cnn(model, images):\n    start = time.time()\n    with torch.no_grad():\n        _ = model(images)\n    end = time.time()\n    return images.size(0) / (end - start + 1e-8)\n\ndef throughput_vit(model, inputs):\n    start = time.time()\n    with torch.no_grad():\n        _ = model(**inputs)\n    end = time.time()\n    return inputs['pixel_values'].size(0) / (end - start + 1e-8)\n\ncnn_tp = throughput_cnn(cnn, images)\nvit_tp = throughput_vit(vit, vit_inputs)\ncnn_params = sum(p.numel() for p in cnn.parameters()) / 1e6\nvit_params = sum(p.numel() for p in vit.parameters()) / 1e6\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "plt.figure(figsize=(5, 3))\nplt.bar(['ResNet-18', 'ViT'], [cnn_params, vit_params])\nplt.title('Parameter Count (M)')\nplt.ylabel('Millions')\nplt.show()\n\nplt.figure(figsize=(5, 3))\nplt.bar(['ResNet-18', 'ViT'], [cnn_tp, vit_tp])\nplt.title('Throughput (images/sec)')\nplt.ylabel('Images/sec')\nplt.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Approx accuracy on a small subset (ImageNet-pretrained models on CIFAR-10)\ndef quick_accuracy_cnn(model, loader, max_batches=5):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for i, (imgs, lbls) in enumerate(loader):\n            if i >= max_batches:\n                break\n            imgs = imgs.to(device)\n            logits = model(imgs)\n            preds = logits.argmax(dim=1).cpu()\n            correct += (preds == lbls).sum().item()\n            total += lbls.size(0)\n    return correct / max(total, 1)\n\ndef quick_accuracy_vit(model, loader, max_batches=5):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for i, (imgs, lbls) in enumerate(loader):\n            if i >= max_batches:\n                break\n            inputs = processor(images=imgs, return_tensors='pt')\n            inputs = {k: v.to(device) for k, v in inputs.items()}\n            logits = model(**inputs).logits\n            preds = logits.argmax(dim=1).cpu()\n            correct += (preds == lbls).sum().item()\n            total += lbls.size(0)\n    return correct / max(total, 1)\n\ncnn_acc = quick_accuracy_cnn(cnn, loader)\nvit_acc = quick_accuracy_vit(vit, loader)\n\nplt.figure(figsize=(5, 3))\nplt.bar(['ResNet-18', 'ViT'], [cnn_acc, vit_acc])\nplt.title('Accuracy (subset)')\nplt.ylabel('Accuracy')\nplt.ylim(0, 1)\nplt.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Find disagreements between models\nimages, labels = next(iter(loader))\nwith torch.no_grad():\n    cnn_logits = cnn(images.to(device))\n    vit_inputs = processor(images=images, return_tensors='pt')\n    vit_inputs = {k: v.to(device) for k, v in vit_inputs.items()}\n    vit_logits = vit(**vit_inputs).logits\ncnn_preds = cnn_logits.argmax(dim=1).cpu()\nvit_preds = vit_logits.argmax(dim=1).cpu()\ndisagree_idx = [i for i in range(len(images)) if cnn_preds[i] != vit_preds[i]]\nfig, axes = plt.subplots(1, 4, figsize=(8, 2))\nfor ax_i, ax in enumerate(axes):\n    if ax_i < len(disagree_idx):\n        idx = disagree_idx[ax_i]\n        ax.imshow(images[idx].permute(1, 2, 0))\n        ax.set_title(f'C:{cnn_preds[idx]} V:{vit_preds[idx]}')\n    ax.axis('off')\nplt.suptitle('CNN vs ViT disagreements')\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Scale Up\n- Evaluate on larger subsets and log memory with torch.cuda.max_memory_allocated.\n- Add a small CNN trained on CIFAR-10 for a fairer comparison.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Pros/Cons"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "| Model | Pros | Cons |\n|---|---|---|\n| CNN | Strong inductive bias, data-efficient | Limited global context |\n| ViT | Global attention, scales with data | Needs large data, compute-heavy |\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## When to use what"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "- Use CNNs for small data, low compute, real-time constraints.\n- Use ViTs when data and compute are ample or for transfer learning at scale.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Discussion prompts"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "1. Why does inductive bias matter for data efficiency?\n2. When is global attention worth the cost?\n3. How do hybrid CNN+Transformer models bridge trade-offs?\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Summary\n- CNNs and ViTs make different assumptions.\n- Empirical throughput and parameter count help choose.\n\n### Exercises\n1. Measure accuracy on the same validation subset.\n2. Try a smaller ViT model and compare throughput.\n3. Find examples where CNN and ViT disagree.\n\n### Further Reading\n- https://arxiv.org/abs/2010.11929 (ViT)\n- https://arxiv.org/abs/2012.12877 (Swin)\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}