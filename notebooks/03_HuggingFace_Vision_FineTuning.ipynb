{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](YOUR_COLAB_LINK_HERE)\n\n# 03 Hugging Face Vision Fine-Tuning\n## Objectives\n- Use datasets + transformers + evaluate.\n- Fine-tune ViT and Swin with Trainer.\n- Save checkpoints and visualize predictions.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "!pip -q install torch torchvision transformers datasets evaluate accelerate matplotlib"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## GPU + mixed precision tips"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Enable GPU in Colab, then set fp16=True in TrainingArguments for faster training."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from datasets import load_dataset\nfrom transformers import AutoImageProcessor, AutoModelForImageClassification, Trainer, TrainingArguments\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\n\ndataset = load_dataset('cifar10')\nprocessor = AutoImageProcessor.from_pretrained('microsoft/swin-tiny-patch4-window7-224')\n\ndef preprocess(example):\n    inputs = processor(example['image'], return_tensors='pt')\n    example['pixel_values'] = inputs['pixel_values'][0]\n    example['labels'] = example['label']\n    return example\n\ndataset = dataset.with_transform(preprocess)\nlabels = dataset['train'].features['label'].names\n\nmodel = AutoModelForImageClassification.from_pretrained(\n    'microsoft/swin-tiny-patch4-window7-224',\n    num_labels=len(labels),\n    id2label={i: l for i, l in enumerate(labels)},\n    label2id={l: i for i, l in enumerate(labels)},\n)\n\ndef collate_fn(batch):\n    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n    labels = torch.tensor([item['labels'] for item in batch])\n    return {'pixel_values': pixel_values, 'labels': labels}\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=1)\n    return {'accuracy': (preds == labels).mean()}\n\nargs = TrainingArguments(\n    output_dir='../outputs/swin_finetune',\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=1,\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    fp16=torch.cuda.is_available(),\n    remove_unused_columns=False,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=dataset['train'].select(range(2000)),\n    eval_dataset=dataset['test'].select(range(500)),\n    data_collator=collate_fn,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\ntrainer.save_model('../outputs/swin_finetune')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## How to adapt to your own dataset"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "- Use `datasets.load_dataset('imagefolder', data_dir=...)`.\n- Update label mappings from dataset features.\n- Adjust image processor size or normalization."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Detection demo: Faster R-CNN (CNN) vs DETR (Transformer)\nWe'll run inference on a Penn-Fudan Pedestrian sample and draw boxes.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import torchvision\nfrom torchvision.transforms import functional as F\nfrom PIL import Image\n\ndataset_det = torchvision.datasets.PennFudanPed(root='../data', download=True)\nimg, _ = dataset_det[0]\nimg_tensor = F.to_tensor(img)\n\n# Faster R-CNN (CNN-based)\nfrcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True).eval()\nwith torch.no_grad():\n    preds = frcnn([img_tensor])[0]\n\nboxes = preds['boxes'][:3].cpu().numpy().tolist()\nlabels = ['person'] * len(boxes)\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1, figsize=(5, 5))\nax.imshow(img)\nfor box in boxes:\n    x0, y0, x1, y1 = box\n    rect = plt.Rectangle((x0, y0), x1 - x0, y1 - y0, fill=False, color='lime', linewidth=2)\n    ax.add_patch(rect)\nax.set_title('Faster R-CNN predictions')\nax.axis('off')\nplt.show()\n\n# DETR (Transformer-based) via Hugging Face\nfrom transformers import DetrForObjectDetection, DetrImageProcessor\nprocessor = DetrImageProcessor.from_pretrained('facebook/detr-resnet-50')\ndetr = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-50').eval()\ninputs = processor(images=img, return_tensors='pt')\nwith torch.no_grad():\n    outputs = detr(**inputs)\ntarget_sizes = torch.tensor([img.size[::-1]])\nresults = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.7)[0]\nfig, ax = plt.subplots(1, 1, figsize=(5, 5))\nax.imshow(img)\nfor score, label, box in zip(results['scores'], results['labels'], results['boxes']):\n    x0, y0, x1, y1 = box.tolist()\n    rect = plt.Rectangle((x0, y0), x1 - x0, y1 - y0, fill=False, color='cyan', linewidth=2)\n    ax.add_patch(rect)\n    ax.text(x0, y0, f'{detr.config.id2label[label.item()]}:{score:.2f}', color='white', fontsize=8, bbox=dict(facecolor='black', alpha=0.6))\nax.set_title('DETR predictions')\nax.axis('off')\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Segmentation demo: DeepLabV3 (CNN) vs SegFormer (Transformer)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from torchvision.datasets import OxfordIIITPet\nfrom torchvision.transforms import functional as F\nimport numpy as np\n\ndataset_seg = OxfordIIITPet(root='../data', download=True, target_types='segmentation')\nimg, mask = dataset_seg[0]\nimg_tensor = F.to_tensor(img)\n\ndeeplab = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True).eval()\nwith torch.no_grad():\n    out = deeplab(img_tensor.unsqueeze(0))['out']\npred_mask = out.argmax(dim=1)[0].cpu().numpy()\n\nplt.figure(figsize=(10, 3))\nplt.subplot(1, 3, 1)\nplt.imshow(img)\nplt.title('Image')\nplt.axis('off')\nplt.subplot(1, 3, 2)\nplt.imshow(mask, cmap='gray')\nplt.title('GT Mask')\nplt.axis('off')\nplt.subplot(1, 3, 3)\nplt.imshow(pred_mask, cmap='gray')\nplt.title('DeepLabV3 Pred')\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n\nfrom transformers import SegformerForSemanticSegmentation, SegformerImageProcessor\nprocessor = SegformerImageProcessor.from_pretrained('nvidia/segformer-b0-finetuned-ade-512-512')\nsegformer = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-ade-512-512').eval()\ninputs = processor(images=img, return_tensors='pt')\nwith torch.no_grad():\n    outputs = segformer(**inputs)\nlogits = outputs.logits\npred = logits.argmax(dim=1)[0].cpu().numpy()\nplt.figure(figsize=(5, 4))\nplt.imshow(img)\nplt.imshow(pred, alpha=0.5, cmap='jet')\nplt.title('SegFormer Overlay')\nplt.axis('off')\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Scale Up\n- Fine-tune for 5+ epochs on a larger subset.\n- Try mixed precision and gradient accumulation.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Summary\n- Hugging Face Trainer handles logging, eval, and checkpoints.\n- Swin is hierarchical and scales well to images.\n\n### Exercises\n1. Fine-tune ViT vs Swin on the same subset.\n2. Try fp16 vs full precision and compare speed.\n3. Load SegFormer and run inference on a pet image.\n\n### Further Reading\n- https://huggingface.co/docs/transformers\n- https://huggingface.co/docs/datasets\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}