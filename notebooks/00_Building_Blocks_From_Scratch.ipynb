{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](YOUR_COLAB_LINK_HERE)\n\n# 00 Building Blocks From Scratch\n## Objectives\n- Implement a tiny CNN and a tiny ViT step-by-step.\n- Track tensor shapes at every stage.\n- Visualize filters, feature maps, and attention.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "!pip -q install torch torchvision matplotlib"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## A) Setup + CIFAR-10 data"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import torch\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, Subset\nimport matplotlib.pyplot as plt\n\ntorch.manual_seed(42)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntransform = transforms.Compose([transforms.ToTensor()])\ntrain_ds = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=transform)\ntest_ds = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=transform)\n\ntrain_subset = Subset(train_ds, range(2000))\ntest_subset = Subset(test_ds, range(500))\n\ntrain_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_subset, batch_size=64, shuffle=False)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "classes = train_ds.classes\nimages, labels = next(iter(train_loader))\nfig, axes = plt.subplots(2, 5, figsize=(8, 4))\nfor i, ax in enumerate(axes.flat):\n    img = images[i].permute(1, 2, 0)\n    ax.imshow(img)\n    ax.set_title(classes[labels[i]])\n    ax.axis('off')\nplt.suptitle('CIFAR-10 Batch')\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## B) Build a tiny CNN step-by-step"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "We start with a single convolution, then add pooling, another block, and a classifier."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import torch.nn as nn\n\ndef shape_summary(model, x):\n    print('Input:', x.shape)\n    for name, layer in model.named_children():\n        x = layer(x)\n        print(f'{name}: {x.shape}')\n    return x\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Step 1: Conv + ReLU\ncnn_step1 = nn.Sequential(\n    nn.Conv2d(3, 8, kernel_size=3, padding=1),\n    nn.ReLU(inplace=True),\n)\nshape_summary(cnn_step1, images[:1])\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Step 2: Add pooling\ncnn_step2 = nn.Sequential(\n    nn.Conv2d(3, 8, kernel_size=3, padding=1),\n    nn.ReLU(inplace=True),\n    nn.MaxPool2d(2),\n)\nshape_summary(cnn_step2, images[:1])\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Step 3: Add a second conv block\ncnn_step3 = nn.Sequential(\n    nn.Conv2d(3, 8, kernel_size=3, padding=1),\n    nn.ReLU(inplace=True),\n    nn.MaxPool2d(2),\n    nn.Conv2d(8, 16, kernel_size=3, padding=1),\n    nn.ReLU(inplace=True),\n    nn.MaxPool2d(2),\n)\nshape_summary(cnn_step3, images[:1])\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Step 4: Add classifier\ncnn = nn.Sequential(\n    nn.Conv2d(3, 8, kernel_size=3, padding=1),\n    nn.ReLU(inplace=True),\n    nn.MaxPool2d(2),\n    nn.Conv2d(8, 16, kernel_size=3, padding=1),\n    nn.ReLU(inplace=True),\n    nn.MaxPool2d(2),\n    nn.Flatten(),\n    nn.Linear(16 * 8 * 8, 10),\n)\nshape_summary(cnn, images[:1])\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Train for 1 epoch"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "cnn = cnn.to(device)\noptimizer = torch.optim.Adam(cnn.parameters(), lr=1e-3)\nloss_fn = nn.CrossEntropyLoss()\ntrain_losses = []\n\ncnn.train()\nfor images, labels in train_loader:\n    images, labels = images.to(device), labels.to(device)\n    optimizer.zero_grad()\n    logits = cnn(images)\n    loss = loss_fn(logits, labels)\n    loss.backward()\n    optimizer.step()\n    train_losses.append(loss.item())\n\nplt.figure(figsize=(5, 3))\nplt.plot(train_losses)\nplt.title('CNN Training Loss (1 epoch)')\nplt.xlabel('Step')\nplt.ylabel('Loss')\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Visualize filters + feature maps"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "weights = cnn[0].weight.data.clone()\nweights = (weights - weights.min()) / (weights.max() - weights.min() + 1e-8)\nfig, axes = plt.subplots(2, 4, figsize=(6, 3))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(weights[i].permute(1, 2, 0).cpu())\n    ax.axis('off')\nplt.suptitle('First-layer filters')\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Feature maps from first conv layer\nwith torch.no_grad():\n    feat_maps = cnn[0](images[:1].to(device)).cpu()\nfig, axes = plt.subplots(2, 4, figsize=(6, 3))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(feat_maps[0, i], cmap='viridis')\n    ax.axis('off')\nplt.suptitle('Feature maps (layer 1)')\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## C) Build a tiny ViT step-by-step"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import math\n\nclass Patchify(nn.Module):\n    def __init__(self, patch_size=4):\n        super().__init__()\n        self.patch_size = patch_size\n\n    def forward(self, x):\n        b, c, h, w = x.shape\n        p = self.patch_size\n        x = x.unfold(2, p, p).unfold(3, p, p)\n        x = x.permute(0, 2, 3, 1, 4, 5)\n        x = x.reshape(b, -1, c * p * p)\n        return x\n\npatchify = Patchify(patch_size=4)\npatches = patchify(images[:1])\nprint('Patches shape:', patches.shape)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "class TinyMHSA(nn.Module):\n    def __init__(self, embed_dim=64, num_heads=4):\n        super().__init__()\n        assert embed_dim % num_heads == 0\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n        self.proj = nn.Linear(embed_dim, embed_dim)\n\n    def forward(self, x):\n        b, n, d = x.shape\n        qkv = self.qkv(x).reshape(b, n, 3, self.num_heads, self.head_dim)\n        qkv = qkv.permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        out = attn @ v\n        out = out.transpose(1, 2).reshape(b, n, d)\n        return self.proj(out), attn\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "class TinyTransformer(nn.Module):\n    def __init__(self, embed_dim=64, num_heads=4, mlp_ratio=2.0):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.attn = TinyMHSA(embed_dim, num_heads)\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, int(embed_dim * mlp_ratio)),\n            nn.GELU(),\n            nn.Linear(int(embed_dim * mlp_ratio), embed_dim),\n        )\n\n    def forward(self, x):\n        attn_out, attn = self.attn(self.norm1(x))\n        x = x + attn_out\n        x = x + self.mlp(self.norm2(x))\n        return x, attn\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "class ToyViT(nn.Module):\n    def __init__(self, img_size=32, patch_size=4, embed_dim=64, depth=2, num_heads=4, num_classes=10):\n        super().__init__()\n        self.patchify = Patchify(patch_size)\n        self.proj = nn.Linear(3 * patch_size * patch_size, embed_dim)\n        num_patches = (img_size // patch_size) ** 2\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        self.blocks = nn.ModuleList([TinyTransformer(embed_dim, num_heads) for _ in range(depth)])\n        self.norm = nn.LayerNorm(embed_dim)\n        self.head = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, x):\n        patches = self.patchify(x)\n        x = self.proj(patches)\n        cls = self.cls_token.expand(x.size(0), -1, -1)\n        x = torch.cat([cls, x], dim=1)\n        x = x + self.pos_embed\n        attn_maps = []\n        for block in self.blocks:\n            x, attn = block(x)\n            attn_maps.append(attn)\n        x = self.norm(x)\n        logits = self.head(x[:, 0])\n        return logits, attn_maps\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Train the toy ViT for 1 epoch"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "vit = ToyViT().to(device)\noptimizer = torch.optim.Adam(vit.parameters(), lr=3e-4)\nloss_fn = nn.CrossEntropyLoss()\nvit_losses = []\nvit.train()\nfor images, labels in train_loader:\n    images, labels = images.to(device), labels.to(device)\n    optimizer.zero_grad()\n    logits, _ = vit(images)\n    loss = loss_fn(logits, labels)\n    loss.backward()\n    optimizer.step()\n    vit_losses.append(loss.item())\n\nplt.figure(figsize=(5, 3))\nplt.plot(vit_losses)\nplt.title('Toy ViT Training Loss (1 epoch)')\nplt.xlabel('Step')\nplt.ylabel('Loss')\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## D) Attention visualization"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "vit.eval()\nwith torch.no_grad():\n    logits, attn_maps = vit(images[:1].to(device))\nlast_attn = attn_maps[-1].mean(dim=1)[0]  # average heads\nattn_to_patches = last_attn[0, 1:]\nside = int(attn_to_patches.numel() ** 0.5)\nattn_map = attn_to_patches.reshape(side, side).cpu().numpy()\nplt.figure(figsize=(4, 4))\nplt.imshow(attn_map, cmap='inferno')\nplt.title('Attention over patches')\nplt.axis('off')\nplt.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import torch.nn.functional as F\nimg = images[0].permute(1, 2, 0).cpu()\nattn_tensor = torch.tensor(attn_map).unsqueeze(0).unsqueeze(0)\nattn_up = F.interpolate(attn_tensor, size=(32, 32), mode='bilinear', align_corners=False)[0, 0]\nplt.figure(figsize=(4, 4))\nplt.imshow(img)\nplt.imshow(attn_up, cmap='inferno', alpha=0.5)\nplt.title('Attention overlay')\nplt.axis('off')\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## E) Wrap-up"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Scale Up\n- Train for 10-50 epochs and increase embedding size for better results.\n- Add data augmentation like random crop and color jitter.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Summary\n- CNNs bake in locality and translation equivariance.\n- ViTs treat images as token sequences with global attention.\n- The toy ViT is data-hungry and needs more scale.\n\n### Exercises\n1. Change patch size from 4 to 8 and observe shapes and accuracy.\n2. Increase the number of heads or depth.\n3. Add data augmentation and compare losses.\n4. Remove LayerNorm and observe stability.\n\n### Further Reading\n- https://arxiv.org/abs/2010.11929 (ViT)\n- https://pytorch.org/tutorials/\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}