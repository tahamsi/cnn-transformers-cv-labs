{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](YOUR_COLAB_LINK_HERE)\n\n# 02 Vision Transformers Fundamentals\n## Objectives\n- Understand patch embeddings, positional encoding, and self-attention.\n- Fine-tune a pretrained ViT on Tiny ImageNet or CIFAR-10.\n- Visualize attention maps and confusion matrix.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Conceptual Primer: CNNs and Transformers\n\n### What is a CNN?\nA Convolutional Neural Network is a neural network designed for images. It uses **convolutions** to scan local regions with learnable filters. This builds in two key inductive biases: **locality** (nearby pixels matter most) and **translation equivariance** (a pattern can appear anywhere).\n\n**How it works**\n- **Convolution layers** learn filters that detect edges, textures, and shapes.\n- **Pooling** reduces spatial size and increases robustness to small shifts.\n- **Deeper layers** combine simpler patterns into higher-level features.\n- A **classifier head** maps features to labels.\n\nMathematically, convolution computes weighted sums over local patches; stacking layers grows the receptive field. CNNs are data-efficient because their priors match natural images.\n\n### What is a Transformer (ViT) for vision?\nA Vision Transformer treats an image as a **sequence of patch tokens**. Each patch is embedded into a vector, positional information is added, and **self-attention** lets every patch interact with every other patch.\n\n**Architecture overview**\n- **Patchify** the image (e.g., 16x16 patches).\n- **Linear embedding** of each patch.\n- **Positional embeddings** preserve spatial order.\n- **Transformer encoder blocks**: LayerNorm \u2192 Multi-Head Self-Attention \u2192 MLP, with residual connections.\n- **Class token** (or token pooling) feeds a classifier head.\n\nSelf-attention gives global context from the start. ViTs typically need large-scale data or pretraining, but transfer learning makes them practical for smaller datasets.\n\n### Key differences (intuition)\n- **Inductive bias**: CNNs assume locality; Transformers are more flexible but data-hungry.\n- **Context**: CNNs build global context gradually; Transformers use global attention immediately.\n- **Compute**: CNNs are efficient; attention cost grows with image size.\n\nReferences: ViT (https://arxiv.org/abs/2010.11929), Attention Is All You Need (https://arxiv.org/abs/1706.03762), ResNet (https://arxiv.org/abs/1512.03385).\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "!pip -q install torch torchvision transformers datasets evaluate matplotlib"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Quick refresher: attention in one cell"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import torch\nattn = torch.randn(1, 4, 8, 8)\nattn = attn.softmax(dim=-1)\nprint('Attention shape:', attn.shape)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Fine-tune ViT with Hugging Face"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from datasets import load_dataset\nfrom transformers import AutoImageProcessor, ViTForImageClassification, Trainer, TrainingArguments\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\n\ndataset = load_dataset('cifar10')\nprocessor = AutoImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n\ndef preprocess(example):\n    inputs = processor(example['image'], return_tensors='pt')\n    example['pixel_values'] = inputs['pixel_values'][0]\n    example['labels'] = example['label']\n    return example\n\ndataset = dataset.with_transform(preprocess)\nlabels = dataset['train'].features['label'].names\n\nmodel = ViTForImageClassification.from_pretrained(\n    'google/vit-base-patch16-224-in21k',\n    num_labels=len(labels),\n    id2label={i: l for i, l in enumerate(labels)},\n    label2id={l: i for i, l in enumerate(labels)},\n)\n\ndef collate_fn(batch):\n    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n    labels = torch.tensor([item['labels'] for item in batch])\n    return {'pixel_values': pixel_values, 'labels': labels}\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=1)\n    return {'accuracy': (preds == labels).mean()}\n\nargs = TrainingArguments(\n    output_dir='../outputs/vit_finetune',\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=1,\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    logging_steps=50,\n    fp16=torch.cuda.is_available(),\n    remove_unused_columns=False,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=dataset['train'].select(range(2000)),\n    eval_dataset=dataset['test'].select(range(500)),\n    data_collator=collate_fn,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Visualizations\nWe plot training curves, sample predictions, a confusion matrix, and an attention overlay.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "log_history = [x for x in trainer.state.log_history if 'loss' in x]\nlosses = [x['loss'] for x in log_history]\nplt.figure(figsize=(5, 3))\nplt.plot(losses)\nplt.title('Training Loss (HF Trainer)')\nplt.xlabel('Log step')\nplt.ylabel('Loss')\nplt.show()\n\n# Sample predictions\nsample = dataset['test'].select(range(10))\nbatch = collate_fn([sample[i] for i in range(10)])\nwith torch.no_grad():\n    logits = model(batch['pixel_values'])\npreds = logits.logits.argmax(dim=1).cpu()\nfig, axes = plt.subplots(2, 5, figsize=(8, 4))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(sample[i]['image'])\n    ax.set_title(f'T:{labels[sample[i][\"labels\"]]} / P:{labels[preds[i]]}')\n    ax.axis('off')\nplt.suptitle('ViT Sample Predictions')\nplt.tight_layout()\nplt.show()\n\n# Confusion matrix on a small subset\ny_true, y_pred = [], []\nsmall = dataset['test'].select(range(200))\nfor i in range(0, len(small), 16):\n    batch = collate_fn([small[j] for j in range(i, min(i + 16, len(small)))])\n    with torch.no_grad():\n        logits = model(batch['pixel_values'])\n    preds = logits.logits.argmax(dim=1).cpu().tolist()\n    y_pred.extend(preds)\n    y_true.extend([small[j][\"labels\"] for j in range(i, min(i + 16, len(small)))])\nimport torch as _torch\ncm = _torch.zeros(len(labels), len(labels), dtype=_torch.int64)\nfor t, p in zip(y_true, y_pred):\n    cm[t, p] += 1\nplt.figure(figsize=(5, 4))\nplt.imshow(cm, cmap='Blues')\nplt.title('Confusion Matrix (subset)')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.colorbar()\nplt.tight_layout()\nplt.show()\n\n# Attention overlay\nimage = sample[0]['image']\ninputs = processor(image, return_tensors='pt')\nwith torch.no_grad():\n    outputs = model(**inputs, output_attentions=True)\nattn = outputs.attentions[-1].mean(dim=1)[0]\nattn_map = attn[0, 1:]\nside = int(attn_map.numel() ** 0.5)\nattn_map = attn_map.reshape(side, side)\nattn_map = torch.nn.functional.interpolate(attn_map.unsqueeze(0).unsqueeze(0), size=image.size[::-1], mode='bilinear', align_corners=False)[0, 0]\nplt.figure(figsize=(4, 4))\nplt.imshow(image)\nplt.imshow(attn_map, cmap='inferno', alpha=0.5)\nplt.title('Attention Overlay')\nplt.axis('off')\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Scale Up\n- Train for 5-20 epochs and unfreeze all layers.\n- Try larger image sizes (224) and stronger augmentation.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Summary\n- ViTs tokenize images into patches and use self-attention.\n- Positional embeddings encode spatial order.\n- Pretrained models adapt quickly with fine-tuning.\n\n### Exercises\n1. Compare frozen backbone vs full fine-tune.\n2. Try different learning rates and batch sizes.\n3. Visualize attention rollout using model outputs.\n\n### Further Reading\n- https://arxiv.org/abs/2010.11929 (ViT)\n- https://arxiv.org/abs/2012.12877 (Swin)\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}