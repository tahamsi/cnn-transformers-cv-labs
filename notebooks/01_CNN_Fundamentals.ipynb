{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](YOUR_COLAB_LINK_HERE)\n\n# 01 CNN Fundamentals\n## Objectives\n- Understand convolution, padding, stride, and pooling.\n- Train a small CNN and interpret features.\n- Visualize filters, feature maps, and saliency.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Conceptual Primer: CNNs and Transformers\n\n### What is a CNN?\nA Convolutional Neural Network is a neural network designed for images. It uses **convolutions** to scan local regions with learnable filters. This builds in two key inductive biases: **locality** (nearby pixels matter most) and **translation equivariance** (a pattern can appear anywhere).\n\n**How it works**\n- **Convolution layers** learn filters that detect edges, textures, and shapes.\n- **Pooling** reduces spatial size and increases robustness to small shifts.\n- **Deeper layers** combine simpler patterns into higher-level features.\n- A **classifier head** maps features to labels.\n\nMathematically, convolution computes weighted sums over local patches; stacking layers grows the receptive field. CNNs are data-efficient because their priors match natural images.\n\n### What is a Transformer (ViT) for vision?\nA Vision Transformer treats an image as a **sequence of patch tokens**. Each patch is embedded into a vector, positional information is added, and **self-attention** lets every patch interact with every other patch.\n\n**Architecture overview**\n- **Patchify** the image (e.g., 16x16 patches).\n- **Linear embedding** of each patch.\n- **Positional embeddings** preserve spatial order.\n- **Transformer encoder blocks**: LayerNorm \u2192 Multi-Head Self-Attention \u2192 MLP, with residual connections.\n- **Class token** (or token pooling) feeds a classifier head.\n\nSelf-attention gives global context from the start. ViTs typically need large-scale data or pretraining, but transfer learning makes them practical for smaller datasets.\n\n### Key differences (intuition)\n- **Inductive bias**: CNNs assume locality; Transformers are more flexible but data-hungry.\n- **Context**: CNNs build global context gradually; Transformers use global attention immediately.\n- **Compute**: CNNs are efficient; attention cost grows with image size.\n\nReferences: ViT (https://arxiv.org/abs/2010.11929), Attention Is All You Need (https://arxiv.org/abs/1706.03762), ResNet (https://arxiv.org/abs/1512.03385).\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "!pip -q install torch torchvision matplotlib"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import os, sys\nrepo_root = os.path.abspath('..')\nsys.path.insert(0, repo_root)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Conceptual overview"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Convolutions assume locality and translation equivariance. Pooling grows the receptive field and adds robustness."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import torch\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, Subset\nimport matplotlib.pyplot as plt\nfrom src.models.cnn import SimpleCNN\nfrom src.trainers.training import train_one_epoch, evaluate\nfrom src.utils.device import get_device\nfrom src.utils.plotting import plot_curves\nfrom src.vision_utils.gradcam import GradCAM\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "device = get_device()\ntransform = transforms.Compose([transforms.ToTensor()])\ntrain_ds = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=transform)\ntest_ds = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=transform)\ntrain_loader = DataLoader(Subset(train_ds, range(3000)), batch_size=64, shuffle=True)\ntest_loader = DataLoader(Subset(test_ds, range(500)), batch_size=64, shuffle=False)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Train a small CNN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "model = SimpleCNN(num_classes=10).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nhistory = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\nfor _ in range(2):\n    train_stats = train_one_epoch(model, train_loader, optimizer, device)\n    val_stats = evaluate(model, test_loader, device)\n    history['train_loss'].append(train_stats['loss'])\n    history['train_acc'].append(train_stats['acc'])\n    history['val_loss'].append(val_stats['loss'])\n    history['val_acc'].append(val_stats['acc'])\n\nplot_curves({'train_loss': history['train_loss'], 'val_loss': history['val_loss']}, 'CNN Loss')\nplot_curves({'train_acc': history['train_acc'], 'val_acc': history['val_acc']}, 'CNN Accuracy')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Visualize filters and feature maps"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "weights = model.features[0].weight.data.clone()\nweights = (weights - weights.min()) / (weights.max() - weights.min() + 1e-8)\nfig, axes = plt.subplots(2, 4, figsize=(6, 3))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(weights[i].permute(1, 2, 0).cpu())\n    ax.axis('off')\nplt.suptitle('First-layer filters')\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Grad-CAM (saliency)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "images, labels = next(iter(test_loader))\nimages = images.to(device)\ncam = GradCAM(model, model.features[4])\nheatmap = cam(images[:1]).cpu()[0, 0]\nplt.figure(figsize=(4, 4))\nplt.imshow(images[0].permute(1, 2, 0).cpu())\nplt.imshow(heatmap, cmap='jet', alpha=0.5)\nplt.title('Grad-CAM')\nplt.axis('off')\nplt.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "model.eval()\nimages, labels = next(iter(test_loader))\nwith torch.no_grad():\n    logits = model(images.to(device))\n    preds = logits.argmax(dim=1).cpu()\nfig, axes = plt.subplots(2, 5, figsize=(8, 4))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(images[i].permute(1, 2, 0))\n    ax.set_title(f'T:{train_ds.classes[labels[i]]} / P:{train_ds.classes[preds[i]]}')\n    ax.axis('off')\nplt.suptitle('Sample predictions')\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from src.utils.metrics import confusion_matrix\ny_true, y_pred = [], []\nwith torch.no_grad():\n    for images, labels in test_loader:\n        logits = model(images.to(device))\n        preds = logits.argmax(dim=1).cpu().tolist()\n        y_pred.extend(preds)\n        y_true.extend(labels.tolist())\ncm = confusion_matrix(y_true, y_pred, num_classes=10)\nplt.figure(figsize=(5, 4))\nplt.imshow(cm, cmap='Blues')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.colorbar()\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Scale Up\n- Train 10-30 epochs and use data augmentation for better results.\n- Try ResNet-18 for a stronger baseline.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Summary\n- Convolutions encode locality and translation equivariance.\n- Pooling increases receptive fields and reduces spatial resolution.\n- Saliency shows which regions influence predictions.\n\n### Exercises\n1. Change kernel size/stride and compare accuracy.\n2. Add batch normalization and measure stability.\n3. Add augmentation and compare results.\n\n### Further Reading\n- https://arxiv.org/abs/1409.1556 (VGG)\n- https://arxiv.org/abs/1512.03385 (ResNet)\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}